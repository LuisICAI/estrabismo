{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 19:27:18.066386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-03 19:27:18.137613: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-03 19:27:18.157427: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-03 19:27:18.567247: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/pabloperez/anaconda3/envs/mbd31/lib/\n",
      "2023-04-03 19:27:18.567289: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/pabloperez/anaconda3/envs/mbd31/lib/\n",
      "2023-04-03 19:27:18.567293: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "\n",
    "import os, shutil\n",
    "import random\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_dir = \"/home/pabloperez/Repositorios/estrabismo/FotosDefinitivas/negative\"\n",
    "positive_dir = \"/home/pabloperez/Repositorios/estrabismo/FotosDefinitivas/positive\"\n",
    "train_dir = os.path.join(\"/home/pabloperez/Repositorios/estrabismo/FotosDefinitivas/train\")\n",
    "validation_dir = os.path.join(\"/home/pabloperez/Repositorios/estrabismo/FotosDefinitivas/validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de imágenes de entrenamiento en negative:  8\n",
      "Número de imágenes de validación en negative:  2\n",
      "Número de imágenes de entrenamiento en positive:  148\n",
      "Número de imágenes de validación en positive:  38\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "negative_train_dir = os.path.join(train_dir, \"negative\")\n",
    "negative_validation_dir = os.path.join(validation_dir, \"negative\")\n",
    "positive_train_dir = os.path.join(train_dir, \"positive\")\n",
    "positive_validation_dir = os.path.join(validation_dir, \"positive\")\n",
    "\n",
    "os.makedirs(negative_train_dir, exist_ok=True)\n",
    "os.makedirs(negative_validation_dir, exist_ok=True)\n",
    "os.makedirs(positive_train_dir, exist_ok=True)\n",
    "os.makedirs(positive_validation_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Lista de nombres de archivo de las imágenes en los directorios \"negative\" y \"positive\"\n",
    "negative_files = [os.path.join(negative_dir, f) for f in os.listdir(negative_dir) if os.path.isfile(os.path.join(negative_dir, f))]\n",
    "positive_files = [os.path.join(positive_dir, f) for f in os.listdir(positive_dir) if os.path.isfile(os.path.join(positive_dir, f))]\n",
    "\n",
    "# Dividir la lista de nombres de archivo en conjuntos de entrenamiento y validación\n",
    "negative_train_files, negative_validation_files = train_test_split(negative_files, test_size=0.2)\n",
    "positive_train_files, positive_validation_files = train_test_split(positive_files, test_size=0.2)\n",
    "\n",
    "# Copiar imágenes de negative al directorio de entrenamiento\n",
    "for filename in negative_train_files:\n",
    "    destination = os.path.join(negative_train_dir, os.path.basename(filename))\n",
    "    shutil.copy(filename, destination)\n",
    "\n",
    "# Copiar imágenes de negative al directorio de validación\n",
    "for filename in negative_validation_files:\n",
    "    destination = os.path.join(negative_validation_dir, os.path.basename(filename))\n",
    "    shutil.copy(filename, destination)\n",
    "\n",
    "# Copiar imágenes de positive al directorio de entrenamiento\n",
    "for filename in positive_train_files:\n",
    "    destination = os.path.join(positive_train_dir, os.path.basename(filename))\n",
    "    shutil.copy(filename, destination)\n",
    "\n",
    "# Copiar imágenes de positive al directorio de validación\n",
    "for filename in positive_validation_files:\n",
    "    destination = os.path.join(positive_validation_dir, os.path.basename(filename))\n",
    "    shutil.copy(filename, destination)\n",
    "\n",
    "print(\"Número de imágenes de entrenamiento en negative: \", len(negative_train_files))\n",
    "print(\"Número de imágenes de validación en negative: \", len(negative_validation_files))\n",
    "print(\"Número de imágenes de entrenamiento en positive: \", len(positive_train_files))\n",
    "print(\"Número de imágenes de validación en positive: \", len(positive_validation_files))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(300, 300, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5)) # Es para evitar overfitting\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(learning_rate=0.0001),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "n = datetime.datetime.now()\n",
    "print(f'Hora de inicio de celda: {n}')\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,)\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(300, 300),\n",
    "        batch_size=32,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(300, 300),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=10, #100\n",
    "      epochs=10, #100\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=10) #50\n",
    "\n",
    "nf = datetime.datetime.now()\n",
    "print(f'Hora de final de celda: {nf}')\n",
    "print(f'Tiempo total: {nf-n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Modelos/Version1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mbd31",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
